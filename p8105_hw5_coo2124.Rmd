---
title: "p8105_hw5_coo2124"
author: "Christiana Odewumi"
output: github_document
---

# Load necessary libraries 
```{r}
library(tidyverse)    
library(broom)        
library(purrr)
library(rvest)

set.seed(1)

knitr::opts_chunk$set(
        echo = TRUE,
        warning = FALSE,
        fig.width = 6,
        fig.asp = .6,
        out.width = "90%"
)

theme_set (theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```


## problem 1

Function to simulate birthdays and checks whether there are duplicate birthdays in the group
```{r}
simulate_shared_birthday = function(n) {
  birthdays = sample(1:365, n, replace = TRUE)
  has_duplicate = any(duplicated(birthdays))
}
```


Run this function 10,000 times for each group size between 2 and 50.
```{r}
compute_duplicate_probability <- function(n) {
  simulations <- map_lgl(1:10000, ~simulate_shared_birthday(n))
  prob <- mean(simulations)
  
  tibble(
    group_size = n,
    prob_duplicate = prob
  )
}

result_final <- map_dfr(2:50, compute_duplicate_probability)
result_final
```


Make a plot showing the probability as a function of group size, and comment on your results.
```{r}
ggplot(result_final, aes(x = group_size, y = prob_duplicate)) +
  geom_line() +
  geom_point() +
  labs(
    title = "Probability of People Sharing a Birthday",
    x = "Group Size",
    y = "Probability"
  ) +
  theme_minimal()
```

comment 
As the group size (n) increases, the probability of at least two people sharing a birthday rises rapidly, surpassing 50% with just 23 people. By a group of 50, the probability is nearly 100%, making a shared birthday almost certain. The plot highlights a nonlinear relationship, with a sharp increase for smaller groups that gradually levels off as the probability approaches 1, showing diminishing returns beyond 30 people.


## problem 2
```{r}

simulate_t_test <- function(mu, n = 30, sigma = 5) {
  data <- rnorm(n, mean = mu, sd = sigma)
  
  t_test_result <- t.test(data, mu = 0) %>% 
    broom::tidy()
  
  tibble(
    mu_hat = t_test_result$estimate,
    p_value = t_test_result$p.value
  )
}

simulate_for_mu = function(mu, n = 30, sigma = 5, simulations = 5000) {
  results = map_dfr(1:simulations, ~ simulate_t_test(mu, n, sigma))
  
  power = mean(results$p_value < 0.05)
  avg_mu = mean(results$mu_hat)
  avg_mu_rejected = mean(results$mu_hat[results$p_value < 0.05], na.rm = TRUE)
  
  tibble(
    true_mu = mu,
    power = power,
    avg_mu = avg_mu,
    avg_mu_rejected = avg_mu_rejected
  )
}

mu_values = c(1,2,3,4,5,6)
simulation_results <- map_dfr(mu_values, simulate_for_mu)

simulation_results
```


```{r}
ggplot(simulation_results, aes(x = true_mu, y = power)) +
  geom_line(color = "pink") +
  geom_point(color = "purple") +
  labs(
    title = "Power of the Test vs. True Mu",
    x = "True Value of Mu",
    y = "Power"
  ) +
  theme_minimal()
```

comments
The plot shows that as the true value of mu increases, the power of the test rises sharply. For smaller mu, the power is low, but it quickly approaches 1 for larger mu, indicating a higher likelihood of correctly rejecting the null hypothesis as the effect size grows.


```{r}
ggplot(simulation_results, aes(x = true_mu)) +
  geom_line(aes(y = avg_mu), color = "green", linetype = "solid", size = 1) +
  geom_line(aes(y = avg_mu_rejected), color = "orange", linetype = "solid", size = 1) +
  geom_point(aes(y = avg_mu), color = "green") +
  geom_point(aes(y = avg_mu_rejected), color = "orange") +
  labs(
    title = "Average Estimate of Mu vs. True Mu",
    x = "True Value of Mu",
    y = "Average Estimate"
  ) +
  theme_minimal()
```

comments
The sample average of ðœ‡Ì‚ across tests where the null hypothesis was rejected is not approximately equal to the true value ofðœ‡. This is due to selection bias: tests that reject the null (p<0.05) are more likely to come from samples where ðœ‡Ì‚deviates significantly from 0, leading to inflated estimates. This is evident in the plot, where the orange line representing the average of ðœ‡Ì‚for rejected samples consistently lies above the true value ofðœ‡In contrast, the green line, which shows the overall average of ðœ‡Ì‚ across all samples, aligns closely with the true value, demonstrating that the estimation process is unbiased when considering all data. Thus, focusing only on rejected samples results in biased, upwardly skewed estimates that do not accurately reflect the true mean.
  
  

## problem 3

Load data from the GitHub repository (replace with the actual URL of the dataset)
```{r}
homicide_data <- read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

str(homicide_data)
```

comments

The raw dataset contains 52,179 rows and 12 columns, with each row representing a unique homicide case from 50 large U.S. cities. The dataset includes demographic details about the victims, such as their first and last names (victim_first, victim_last), race (victim_race), age (victim_age), and sex (victim_sex). Geographic information is provided through the city and state where the homicide occurred (city, state), as well as latitude and longitude (lat, lon) for mapping purposes. The date the homicide was reported is recorded in the reported_date column, and each case is uniquely identified by the uid column. The disposition column indicates the status of each case, distinguishing between solved cases (e.g., "Closed by arrest") and unsolved cases (e.g., "Open/No arrest"). This dataset provides a comprehensive view of homicide cases, enabling detailed analysis of trends, victim demographics, and case outcomes across major U.S. cities. Some columns, such as victim_age and geographic coordinates, may contain missing values, which should be accounted for in the analysis.


Create city_state variable and summarize homicides
```{r}
homicide_summary = homicide_data %>%
  mutate(city_state = paste(city, state, sep = ", ")) %>%
  group_by(city_state) %>%
  summarize(
    total_homicides = n(),
    unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  ) %>%
  ungroup()


homicide_summary
```


```{r}
baltimore_data = homicide_summary %>% filter(city_state == "Baltimore, MD")
baltimore_test = prop.test(
  baltimore_data$unsolved_homicides,
  baltimore_data$total_homicides
)

baltimore_result = broom::tidy(baltimore_test) %>%
  select(estimate, conf.low, conf.high)

baltimore_result
```

```{r}

city_results = homicide_summary %>%
  mutate(
    prop_test = map2(
      unsolved_homicides,
      total_homicides,
      ~ prop.test(.x, .y) %>% broom::tidy()
    )
  ) %>%
  unnest(prop_test) %>%
  select(city_state, estimate, conf.low, conf.high)

city_results
```

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. 
```{r}
city_results = homicide_summary %>%
  mutate(
    prop_test = map2(
      unsolved_homicides,
      total_homicides,
      ~ prop.test(.x, .y) %>% broom::tidy()
    )
  ) %>%
  unnest(prop_test) %>%
  select(city_state, estimate, conf.low, conf.high)

city_results
```

Create a plot that shows the estimates and CIs for each city â€“ check out geom_errorbar for a way to add error bars based on the upper and lower limits. 
```{r}
ggplot(city_results, aes(x = reorder(city_state, estimate), y = estimate)) +
  geom_point(size = 2, color = "pink") +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.3, color = "purple") +
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City",
    y = "Estimated Proportion of Unsolved Homicides"
  ) +
  coord_flip() +  
  theme_minimal()

```

